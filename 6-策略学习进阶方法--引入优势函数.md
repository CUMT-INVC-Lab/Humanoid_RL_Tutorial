传统的Actor-Critic有一个问题，在状态较好的情况下，所有的动作带来的回报都很高，因此算法会增大所有动作的概率。在状态较差的情况下，所有的动作回报都相对较低，算法就会减小所有动作的概率。实际上，我们想要的是在一个状态下相对于其他动作来说最优的那个动作，因此需要对算法进行一些改进。这里引入一个新的概念，优势函数（Advantage function）：

![image](https://cdn.nlark.com/yuque/__latex/f708afc0c8c00b8d254ade97758476f0.svg)

它是一个状态![image](https://cdn.nlark.com/yuque/__latex/79ce3c7a71877c2ff01695e38ade43ca.svg)下做出动作![image](https://cdn.nlark.com/yuque/__latex/26fdbf8e53cb0e48da5f4ddd4aaf5a5c.svg)的动作价值函数减去其状态价值函数。优势函数可以反应出一个动作相对于在同一状态下其他动作的优劣（因为它们都需要减去一个![image](https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg)，他们的状态越好减去的![image](https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg)越大，在一定程度上可以削减状态的好坏对动作执行概率的影响）。

根据上一节对求梯度做近似的内容，用优势函数去代替原来的动作价值函数，就可以得到新的梯度公式：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730339512329-336c06de-79bc-4bf8-8167-51042adf137d.png)

这种算法就是Advantage Actor-Critic（A2C）算法。在Advantage Actor-Critic算法中，我们不对动作价值函数建立神经网络![image](https://cdn.nlark.com/yuque/__latex/3968613e29507d0ef6eefc645dfecac5.svg)，而是对状态价值函数建立神经网络![image](https://cdn.nlark.com/yuque/__latex/8e99b6f69e3e9c3667b336176b75a6f1.svg)。

## 1.价值网络的训练
根据贝尔曼公式：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730342605682-fd569f09-08df-42e4-9bbb-794e52f88ca6.png)

其中左边的![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)就是我们需要用价值网络![image](https://cdn.nlark.com/yuque/__latex/8e99b6f69e3e9c3667b336176b75a6f1.svg)拟合出来的。类似于上一节对价值网络![image](https://cdn.nlark.com/yuque/__latex/34c7b563b30bde3c748139530686798e.svg)的处理，等式右边的可以用蒙特卡洛近似，用实际值去代替求期望。这样就得到以下式子，这就是TD target：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730342753376-fe63d319-be9b-4ce7-96e8-754db2a830db.png)

这里的![image](https://cdn.nlark.com/yuque/__latex/99ffe0fdf7bc1d0cd5cd1d9efeb912e7.svg)是价值网络![image](https://cdn.nlark.com/yuque/__latex/8e99b6f69e3e9c3667b336176b75a6f1.svg)在t+1时刻对![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)进行的估计（也就是汽车走到一个中转城市后对总时间做的估计），它比t时刻由价值网络对![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)的估计（也就是汽车出发前做的估计）更加准确。因此我们要更新参数![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)使![image](https://cdn.nlark.com/yuque/__latex/4045a1892fe21aef28e7ce896ba2159d.svg)更加接近![image](https://cdn.nlark.com/yuque/__latex/99ffe0fdf7bc1d0cd5cd1d9efeb912e7.svg)。后续的动作就和之前的TD算法一样了，求TD Error，做梯度下降。

定义损失函数：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343386673-31bff05f-07b1-4121-a23a-271353036c04.png)

对损失函数求梯度：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343376077-b10ba658-d38a-43eb-94fd-c53697acb0a0.png)

对参数进行梯度下降更新：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343397682-9cafac15-2c5f-4379-9033-ef61b5463445.png)

## 2.策略网络的训练
由于不再使用动作价值函数去估算梯度，而是使用优势函数，因此策略函数的更新发生了一些变化。

将动作价值函数替换为优势函数后，根据贝尔曼公式：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343819556-f24abce2-0c95-4ef2-a38d-410bef57519c.png)

类似于刚刚价值网络训练的做法，使用蒙特卡洛近似代替求期望后：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343890256-535bd271-078c-4015-a4fa-4f394f0d531c.png)

我们使用了价值网络去估算![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)，所以式子可以写成一下形式：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730343980776-64f87ff9-fbd1-41ce-bf49-d6e79d04eadf.png)

类似于价值网络部分提到的TD target ![image](https://cdn.nlark.com/yuque/__latex/99ffe0fdf7bc1d0cd5cd1d9efeb912e7.svg)和TD Error ![image](https://cdn.nlark.com/yuque/__latex/8fde9bfe6bbd71472999db66a4471f0d.svg):

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730344068444-4de31a7c-fdc3-4546-bbb1-b90b0b992ed4.png)

因此式子又可以被化简成：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730344092688-d6a4a7ed-b5c9-4f36-be0b-2244e4226463.png)

然后就可以使用梯度上升法对策略参数进行更新：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730344119341-b719d459-7530-4ec3-9ffd-2845ea6fa0f7.png)

刚刚在训练价值网络时，发现价值网络是通过训练使自己判断的![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)更加准确，但是![image](https://cdn.nlark.com/yuque/__latex/ad1dada955e9bdb8f6a689668b915085.svg)是状态价值函数，和执行的动作![image](https://cdn.nlark.com/yuque/__latex/26fdbf8e53cb0e48da5f4ddd4aaf5a5c.svg)无关，那他是如何判断动作的好坏呢？其实是用TD Error来大致表示动作的好坏。根据TD Error的定义：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730344748812-e7490b4a-bb0b-48fe-aa27-413243298349.png)

可以看到其包含了![image](https://cdn.nlark.com/yuque/__latex/9712f236a44befc205c27c858ae9657f.svg)，这个![image](https://cdn.nlark.com/yuque/__latex/9712f236a44befc205c27c858ae9657f.svg)就是在执行完动作![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg)之后才能得到的，因此TD Error是包含了动作信息的。如果执行完动作![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg)后得到的![image](https://cdn.nlark.com/yuque/__latex/99ffe0fdf7bc1d0cd5cd1d9efeb912e7.svg)大于执行前的预估值![image](https://cdn.nlark.com/yuque/__latex/4045a1892fe21aef28e7ce896ba2159d.svg)，也就是TD Error小于0，就说明![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg)的收益（![image](https://cdn.nlark.com/yuque/__latex/ea05ff4255e7dbb1f9d66808e18da63f.svg)）大于预期，在更新参数![image](https://cdn.nlark.com/yuque/__latex/ed5a4aa5e092e303a69c608582c70db9.svg)时应当增大其执行概率；如果执行完动作后得到的![image](https://cdn.nlark.com/yuque/__latex/99ffe0fdf7bc1d0cd5cd1d9efeb912e7.svg)小于执行前的预估值![image](https://cdn.nlark.com/yuque/__latex/4045a1892fe21aef28e7ce896ba2159d.svg)，也就是TD Error大于0，就说明![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg)的收益（![image](https://cdn.nlark.com/yuque/__latex/ea05ff4255e7dbb1f9d66808e18da63f.svg)）小于预期，在更新参数![image](https://cdn.nlark.com/yuque/__latex/ed5a4aa5e092e303a69c608582c70db9.svg)时应当减小其执行概率，这一点反应在参数![image](https://cdn.nlark.com/yuque/__latex/ed5a4aa5e092e303a69c608582c70db9.svg)的更新公式上。



