强化学习的目标是通过让机器人与环境的交互来学习一个**策略（Policy，对于机器人来说是一种控制策略，根据当前机器人的姿态如何其转动各个电机）**，使得**智能体（Agent，也就是我们要控制的机器人）**能够在不同状态下采取适当的**行动（Action，也就是指各个电机如何转动）**，以最大化其**奖励（Reward，是我们人为设置的一个奖励机制，比如机器人保持一定的高度就给多少reward）**和长期**回报（Return，回报是指机器人在完成运动前获得的奖励之和）**。

### 强化学习的基本术语
1. **智能体（Agent）**： 智能体是学习和决策的主体，负责在环境中采取行动，也就是我们需要控制的机器人。其目标是通过与环境的交互获得奖励，并学会在不同的状态下选择最优的行动。
2. **环境（Environment）**： 环境是智能体所处的外部世界，也就是机器人的运行场地。智能体通过在环境中执行动作来获得反馈（例如获得我们人为设置的奖励）。环境的状态会根据智能体的行为发生变化，比如机器人走路过程中会使自己在环境中的位置发生变化。
3. **状态（State, **$ s $**）**： 状态是环境当前的描述信息，表示智能体所处的情况，也就是机器人的姿态，电机位置，各个部位受到的力等信息。智能体根据所感知的状态通过策略来做出行动。
4. **观测值（Observation）**：观测值是智能体能够直接获取的状态，是状态的子集。对于机器人来说，状态有很多，比如姿态，电机位置，各个部位收到的力，当前的高度等等都是状态。但是机器人没有传感器去获得每个部位受到的力的大小和当前的高度。因此高度等状态对于机器人来说是不可观测的。所以高度等信息是状态，而不是观测值。而姿态等信息是可以通过IMU获取的，因此姿态信息既是状态，也是观测值。
5. **行动（Action, **$ a $**）**： 行动是智能体对当前状态作出的选择，也就是机器人如何去转动其身上的各个电机。根据智能体的策略，不同状态下可以有不同的可选行动。各个行动都有一个对于的执行概率，智能体通过随机抽样决定执行哪个行动并在环境中执行它。
6. **奖励（Reward, **$ r $**）**： 奖励是环境对智能体采取某个行动后反馈的数值，是我们人为设置的，表示该行动的好坏。比如机器人保持一定的高度就给多少奖励，走多长的距离给多少奖励。智能体的目标是最大化其在整个过程中获得的累积奖励，其通过奖励的多少来优化自己的策略。奖励是智能体从我们这里获得的唯一反馈，所以是调参的重点之一。
7. **策略（Policy, **$ π $**）**： 策略是智能体在每个状态下选择行动的规则。对于我们的机器人来说，策略就是机器人在什么姿态下应该如何去转动各个电机。训练好的策略是一个神经网络（通俗一点讲，是一种映射关系），它会把当前的状态映射成行动。策略就是我们最终想要得到的东西。
8. **值函数（Value Function）**： 值函数评估每个状态或状态-行动对的"好坏"，通常以长期回报的预期值来表示。强化学习中常见的两类值函数：
    1. **State value function 状态价值函数 **$ V_\pi(s) $: 表示从状态 s 开始，使用策略$ \pi $，智能体在未来能够获得的预期回报（总奖励）。也就是机器人处于某个姿态下，根据当前策略采取所有可能的行动能够获得的奖励之和的平均值，在策略一致的情况下，它用来反映当前机器人的姿态的好坏。比如直立的姿态就比已经倒了一半的姿态要好，其状态价值函数就会比倒了一半的姿态的状态价值函数高。在状态一致的情况下，策略越好，状态价值函数就越高。
    2. **Action value function 动作价值函数**$ Q_\pi(s,a) $: 表示从状态 s 采取行动 a 后，智能体未来能够获得的预期回报。也就是机器人处于某个姿态下，采取一个动作a，并且以后都按照当前的这个策略$ \pi $来采取行动，它能够获得的奖励之和的平均值。这个值用来反映当前机器人采取的这个行动的好坏，比如同样在直立状态下，往前走一步这个行动就比往后倒这个行动的动作价值函数高（这只是个例子，实际上一个行动所包含的运动量是非常小的）。
9. **State transition function 状态转移函数**$ p_t(s' \mid s, a) = \mathbb{P}(S_{t+1} = s' \mid S_t = s, A_t = a), $表示在状态s下，采取动作a有多大的概率进入状态$ s' $。
10. **回报（Return, **$ G $**）**： 回报是智能体从当前时刻起到未来某个时间点累积获得的奖励，也就是每一个行动带来的奖励之和。由于我们通常更加关注短期奖励（比如现在给你100块钱和一年后给你100块钱让你选一个，你肯定会选现在给你100块钱），因此计算回报的时候通常包含折扣因子，变成折扣回报（下一节开头会细说），越远的奖励的加权值越低。
11. **折扣因子（Discount Factor, **$ γ $**）**： 折扣因子 $ γ $ 是一个介于 0 和 1 之间的数，用来权衡当前奖励和未来奖励的相对重要性。较大的$ γ $ 使得智能体更看重长期回报，而较小的$ γ $ 则更重视当前回报。
12. **探索与利用（Exploration vs. Exploitation）**： 探索是指智能体尝试新行动，以获取更多对环境的了解；利用是指智能体基于已有知识选择最优行动，以最大化奖励。强化学习算法通常需要在探索与利用之间找到平衡。我们机器人用的PPO算法，不用我们去手动设置探索与利用的权重。
13. **轨迹（trajectory）：**轨迹是指智能体完成一个训练回合（episode）经过的所有状态s，采取的所有动作a和得到的所有回报r的集合。对于我们机器人来说，一个训练回合就是机器人在一次训练过程中，在倒下之前的所有时刻的合集。其轨迹就是在倒下之前所有经过的姿态，采取的动作和获得的回报所组成的一个一维数组。

### 强化学习基本算法思想
大致先有个概念，后续讲细节。

1. **价值学习（Value based learning）**： 通过反复更新状态值函数，直到收敛为止，从而得到最优策略，比如DQN。价值迭代的基本思想是去训练一个非常准确的Q（比如用神经网络去训练），这样我们就能知道什么样的a能够使Q最大，然后采取能够使Q最大的那个动作a。
2. **策略学习（Policy based learning）**： 包含两个步骤：首先通过评估当前策略得到状态值函数，然后基于这个状态值函数改进策略，直到策略不再变化。策略迭代的基本思想是直接训练一个策略$ π $,直接通过当前状态去选择对应的动作。

### 强化学习控制机器人的大致过程
我们先设置一个奖励机制，比如机器人保持一定的高度给多少奖励，腿按照我们写好的轨迹运动给多少奖励等等。然后在仿真环境（Issac gym）中同时生成很多个机器人（比如2048个），让它们先随机采取各种动作（电机乱转）直到倒下，然后计算自己在倒下之前一共获得了多少奖励，根据获得了多少奖励，机器人的姿态发生了哪些变化和本次运行过程中采取了哪些行动（电机怎么转的）来计算在什么姿态下采取什么样的行动可以让奖励更高，下一次就以更大的概率在对应姿态下采取对应行动。经过多次迭代，在不同姿态下采取的能使奖励更大的行动就会相对固定（不是绝对固定，也有概率采取不同的行动）。这种从不同姿态到对应的电机转动方式的映射就是我们想要得到的策略（在什么状态下应该采取什么样的行动），得到这个策略后，我们就可以用物理引擎更加真实的仿真环境（Mujoco）来验证我们刚刚得到的策略是否有可能能在真实的机器人上使用。经过Mujoco的验证后，就可以把策略部署在真实的机器人上。

在仿真环境中，软件会输出机器人的各种状态参数，比如各个电机的位置，机器人的坐标和姿态，各个部件受到的力的大小等等。部署到实际的机器人上后，机器人身上的传感器也会给我们提供这些信息，但是就不会这么详细。比如我们可以通过IMU读取机器人的姿态数据，可以通过电机编码器得到各个电机的位置，但是机器人各个部件受到的力是无法被测量的，所以生成的策略不能要求这些力相关变量作为输入，它们仅仅能在训练过程中起辅助作用。这种仿真中的数据和实际机器人的数据的差异就是状态（State）和观测（Observation）的差异，前面已经提到过了。

由于实物机器人身上的传感器不是绝对准确的，会有噪声信号，且机器人的机械装配也不是绝对精准的，质心位置可能会发生偏移。我们在训练机器人的策略的过程中就需要考虑到这些因素，因此在训练的过程中手动引入随机噪声（Random domain）来使得仿真环境的机器人也是会受到噪声影响的。这样训练出来的策略就也会考虑到随机噪声对机器人造成的影响，从而提高其部署到实物机器人后的控制效果。

