## 线性模型拟合
在经典控制理论中，对于一个模型进行建模最简单的办法就是建立一个一阶的线性模型，数学表达式可以表示为：

![image](https://cdn.nlark.com/yuque/__latex/1e49ac0d9b15365dbd0f255faaa09d2f.svg)

其中![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)和![image](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg)就是需要被算出来的参数。

如果![image](https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg)是一个向量，那么![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)也必须是一个向量，里面的元素就是不同![image](https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg)元素的权重。在这种情况下，式子需要被改成：

![image](https://cdn.nlark.com/yuque/__latex/6b638f02c5a71a7fae5ac6dee8047955.svg)

我们训练一个模型，就是去得到参数![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)和![image](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg)，使得这个模型的输入输出和真实模型的输入输出一致（或者非常接近）。

为了实现这个目标，定义一个损失函数，这个损失函数就是在相同的输入下，我们的模型的输出和真实的模型的输出的差异。只要使这个**损失函数最小**，那说明我们的模型和真实的模型的输出差异就很小，就说明我们的模型非常接近真实的模型。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729842093257-34d9af26-72e5-4c4b-a8fe-51c9ba07beb2.png)

到这里就有点像传统控制里面的最优控制问题了，这里的![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)跟最优控制里的cost function![image](https://cdn.nlark.com/yuque/__latex/308ed96aa201ec8fe47df94d5afafe88.svg)的含义差不多。

## 梯度下降
刚刚在线性模型中提到了需要最小化损失函数来使我们的模型更加准确，具体的方式就是使用梯度下降法。

求梯度类似于求偏导，一个目标函数![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)（被求梯度的函数）关于一个变量![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)的梯度可以表示成：

![image](https://cdn.nlark.com/yuque/__latex/394fb05af9f547732544e3fea110b407.svg)

简单来说，梯度是![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)上升的方向，沿着梯度对![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)进行更新会让![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)的值增加。相反，沿着梯度的反方向对![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)进行更新会让![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)的值减小。因此为了让损失函数最小，我们把损失函数作为目标函数![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)，让其对![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)和![image](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg)求梯度，并对![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)沿着梯度进行方向更新，得到的![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)就能让![image](https://cdn.nlark.com/yuque/__latex/c895173d3be4872abf206be4268a58cb.svg)减小，从而得到与真实模型更加接近的![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)。

![image](https://cdn.nlark.com/yuque/__latex/9003eedc793f88e90e304472c5759968.svg)

可以用一个最简单的例子来理解梯度下降，如图所示的一个函数，我们用梯度下降找到它的最小值。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730367049861-9c7b8c2b-8c18-48d8-a45e-10dad9d31b23.png)

假设最初的变量位于函数最左侧的点A，此时函数处于下行阶段，导数小于0，根据梯度下降公式，变量会变大，因此会向右移动。导数绝对值越大，移动的距离越远，具体移动多远由系数![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)决定。由于函数长期处于下行阶段，因此这个变量会不断地向右移动，直到点B。此时导数的绝对值太大，导致点向右移动太多，到了点C。C点函数处于上行阶段，导数大于0，会让点往左移动，于是又到了D点。循环往复，函数会逐渐到达最低点F。这就是梯度下降找最小值的原理。

但是该算法有个问题，严格来讲它找的是极小值而不是最小值，因此可能会陷入局部最优，例如下面的这种情况：

![](https://cdn.nlark.com/yuque/0/2024/webp/49555563/1730367296357-12753238-ac3d-46c4-9795-4470ee1b7a9e.webp)

还有一个问题，就是如果![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)设置过大，可能会导致其一直在极小点左右来回震荡，一直达不到极小值点。如果![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)设置过小，它每一次更新只会移动很小一段距离，因此消耗的时间就会更长。因此设置![image](https://cdn.nlark.com/yuque/__latex/18d25ca4f77a9bbed9812e2bb0b350a5.svg)需要综合考虑收敛速度和收敛精度。

## 神经网络
### 1.全连接层
全连接层就是一种线性的映射关系加上一个激活函数。把![image](https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg)线性映射成一个数据![image](https://cdn.nlark.com/yuque/__latex/eb647136f775841061532ea06e513751.svg)后再用一个激活函数对其进行处理得到![image](https://cdn.nlark.com/yuque/__latex/0e8831d88c93179dbe6c8b5e3678ca20.svg)。其数学表达式如下：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729846026897-e031169b-f28e-40ac-aed7-8c4b358ea14f.png)

对于激活函数，我们一般选用ReLU激活函数，作用是去掉所有的负数。ReLU的表达式如下：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729846117977-087bba4d-8cae-47e4-b7a5-9923a944f0b7.png)

把很多个全连接层首位串联起来就变成了全连接神经网络，也叫**多层感知器（MLP）**。每一个全连接层里面的参数![image](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg)和![image](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg)都不一样。全连接层网络类似于传统控制里的多个一阶系统串联起来变成了多阶系统，有几个全连接层串联就是几阶系统。

全连接层的作用类似于系统辨识，用多个线性模型串联来描述出想要得到的输入输出关系。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729846435793-254d152a-0d0d-42eb-acf5-f84e9d997a13.png)

### 2.卷积层
卷积层的作用一般是用来作**特征提取**。卷积层的输入和输出都是张量（tensor），最后一个卷积层的输出需要向量化，把张量变成向量。卷积层的输出就是输入张量的特征，类似于压缩信息，把很大的张量压缩成很小的向量，具体原理不需要知道，只用知道它是用来提取特征的就行。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729846820237-036a9dbb-d6e2-496b-b4d6-9838fdddd824.png)

### 3.Softmax分类器
我们最终训练出来的机器人控制策略的输出是一个概率分布，它包含了机器人每一个动作的执行概率是多少。比如10%概率抬右腿，80%概率抬左腿，10%概率跳一下，机器人会根据概率随机选一个动作进行执行。为了实现这个输出，就要通过Softmax分类器。

Softmax的作用的直观图如下，它把前面的层输出出来的各个元素的“分值”变成大小关系不变且相加等于1的概率值。那些负数都会被转化成正数（因为概率不能为负），但是非常小。

### ![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729843611828-9bea7c56-7980-4bf7-ad61-ff8200ccde33.png)
比如我们现在要对手写数字进行分类，每一个手写数字都是一个28*28的矩阵（28*28像素的图片）。要训练它，首先要把它展开成1*784的向量（卷积层干这事），神经网络的作用就是把这个1*784的矩阵映射成一个1*10的向量，这个向量里面的10个元素就分别是这个图片里面写的是0还是1还是2还是3......到9的概率。Softmax分类器在最后一步起作用，把神经网络前面的层输出的有正有负的数值变成概率值，大概长下面这个样子：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729843814965-0ce6408b-21a9-49cd-b5bf-aa07c543d7a0.png)

每一个元素的位置代表了它表示的数字，元素的值表示识别结果是这个数字的概率。

### 4.神经网络流程
完整的神经网络识别流程如下图所示，对于一个信息量特别大的数据，先用卷积层进行特征提取，得到一个一维的向量，然后把这个一维的向量输入进全连接网络进行多次线性拟合来模拟目标系统的大致参数，最后用Softmax函数把输出的内容转化成概率分布，就可以得到策略函数。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1729846859879-d677d131-3f35-4e2d-90e8-7db0095a1c28.png)



