我们的机器人使用的是策略学习，因此学习策略学习的概念更加重要。

## 1.策略网络
策略函数$ \pi $的定义是，在状态$ s $下，执行动作$ a $的概率是多少。其数学表达式是：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730167275118-b911cf68-2902-4b1f-8b70-24ee047b9f0c.png)

策略函数的输入是状态和动作，输出是一个0-1的数字，表示在该状态下执行该动作的概率。比如玩超级玛丽，动作有向上，向右，向左三种，则给定状态$ s $的情况下，策略函数分别为：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730167576971-4b03593d-2edf-429d-a158-1372aa741379.png)

具体要执行哪一个动作，需要通过按照概率随机采样来执行动作，并不是直接执行概率最大的动作。

类似于上一节的用一个神经网络$ Q(s,a;w) $去近似拟合$ Q(s,a) $，我们也可以用神经网络$ \pi(a|s;\theta) $去近似拟合$ \pi(a|s) $，其中$ \theta $就是神经网络的参数，是需要被训练更新的变量。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730168133426-6e127a97-7ea7-4b57-a3a0-1047f2da03ca.png)

可以看到这里比上一节的价值网络多了一个Softmax函数，这是因为输出的策略是执行每个动作的概率，之前已经提到了Softmax的作用是把一组数据按照大小顺序不变，全部变成处于0-1的数值且相加等于1。因此策略网络需要使用Softmax函数。

## 2.策略学习的目标函数
复习一下之前提到的一个术语：

**State value function 状态价值函数 **$ V_\pi(s) $: 表示从状态 s 开始，使用策略$ \pi $，智能体在未来能够获得的预期回报（总奖励）。也就是机器人处于某个姿态下，根据当前策略采取所有可能的行动能够获得的奖励之和的平均值，在策略一致的情况下，它用来反映当前机器人的姿态的好坏。比如直立的姿态就比已经倒了一半的姿态要好，其状态价值函数就会比倒了一半的姿态的状态价值函数高。在状态一致的情况下，策略越好，状态价值函数就越高。我们要利用的就是第二个特性。

状态价值函数的数学表达式是：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730170865732-7533ece4-d13b-4492-b4fb-e773f731d308.png)

如果一个策略很好的话，那么状态价值函数的均值应该会很大，所以我们可以通过以状态价值函数的均值定义一个目标函数，通过目标函数的大小来反应我们策略的好坏：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730171205601-90b2fb5b-6023-4847-aff4-c99554a5a67d.png)

通过对策略函数求均值，排除了状态$ S $的不确定性（由于策略的不确定性和状态转移函数的不确定性，状态$ S $是不确定的），因此目标函数$ J $就只与$ \pi $和$ \theta $有关。所以优化策略的问题可以转换成如何让目标函数$ J $最大的问题，这就又变成一个最优控制问题了，找到能够使$ J(\theta) $最大的$ \theta $。

之前提到了可以用梯度下降的方式找到能够使目标函数最小的参数，反过来，也可以用**梯度上升**的方式找到使目标函数最大的参数。让$ J $对参数$ \theta $求梯度，并沿着梯度正方向对$ \theta $进行更新，就能够找到使得让$ J $最大的$ \theta $的值。这就是**策略梯度**算法。

## 3.策略梯度算法
在强化学习中，我们希望模型能采取**一系列能够获得最大化累积回报的动作**。为此，策略梯度算法通过直接优化策略，使得在不同状态下采取某些动作的概率能够带来更高的期望回报。

其具体的实现方式正如刚刚提到的，通过**梯度上升**的方式，找到能够使目标函数$ J $最大的参数$ \theta $来使得状态价值函数最大，也就是说在相同状态下，策略是最优的。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730172620716-5f5b2c60-38ff-4706-97d5-5a6c05c8865f.png)

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730172631507-24a24b38-ab89-4e8b-b512-fc90d6ae9297.png)

其中$ J $对$ \theta $求梯度的具体方式是一个数学变化，公式如下。不用知道具体细节，pytorch能够直接对其求梯度，我们不用管底层数学逻辑。

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730172715851-a4bd1122-4a22-413c-b990-a1aef73c5aa5.png)

这个式子也可以写成一个更加简洁的形式：

$ \nabla J(\theta) = \mathbb{E} \left[ \nabla_\theta \ln \pi(A | S;\theta) \cdot Q_\pi(S,A) \right]
 $

我们发现，在上面求梯度的公式中，需要对后面那一大堆式子求期望，这个期望我们是不能得到的，所以就需要用**蒙特卡洛近似**代替求期望。

因此我们对状态$ S $进行采样，也就是从环境中检测到当前的状态，即为$ s_t $，然后根据当前策略网络随机抽样得到一个动作$ a_t $，然后我们就可以把这两个具体的数据带入公式作为其期望值：

$ g(s_t,a_t;\theta)=\nabla_\theta \ln \pi(a_t | s_t;\theta) \cdot Q_\pi(s_t,a_t) $

这个$ g(s_t,a_t;\theta) $就可以作为梯度$ \nabla J(\theta) $来使用，这样就没有了求期望这个步骤。然后就可以用下面的公式来对参数进行梯度更新：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730188743171-ec15e531-f81e-4944-af5a-91d950e66115.png)

到这里，我们虽然已经使用了$ g(s_t,a_t;\theta) $来去掉求期望的步骤，但是求$ g(s_t,a_t;\theta) $的公式同时也用到了动作价值函数$ Q_\pi(s_t,a_t) $，我们是不知道这个的，因此也需要使用**REINFORCE**算法对$ Q_\pi(s_t,a_t) $进行蒙特卡洛近似。

## 4.REINFORCE算法
REINFORCE 是一种经典的策略梯度算法，其利用“**蒙特卡洛近似**”的方式，采样完整的回合并计算回报$ u_t $去近似$ Q_\pi(s_t,a_t) $，然后将回报与策略的梯度相乘，以增加产生高回报的动作的概率、减少低回报动作的概率。其目标是最大化每个状态下的期望回报。  

### REINFORCE更新梯度流程
1. 首先使用现有的策略（参数是$ \theta_{now} $）控制智能体从头开始运行一局训练（比如玩一局游戏），得到一条轨迹$ [s_1,a_1,r_1,s_2,a_2,r_2.....s_n,a_n,r_n] $。
2. 根据轨迹中包含的奖励信息，将它们全部加起来并乘以对应的折扣因子得到本局的回报$ u_t=\sum_{k=t}^n \gamma^{k-t}r_k $。
3. 使用公式对参数进行梯度更新：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730185585408-3604d19b-ac79-46b0-a1d8-5c9ec4e9277c.png)

为什么这里多出来了一个$ \gamma^{t-1} $项和一个求和项？因为在使用蒙特卡洛近似对$ \nabla J(\theta) $做期望近似时只采样一次来估计均值肯定是不够严谨的，更加严谨的做法是一直采样完一整个轨迹，还要考虑到未来奖励的折扣因子对总奖励的影响，因此两者严格意义上的关系是：

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730189703831-778183d6-6d10-454e-a1bc-6b65666007ee.png)

                               ![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730189777357-52a21815-df8e-491d-8758-5c09ed9537fb.png)

于是就有了上面的公式。

## 5.Actor-Critic算法
在REINFORCE算法中，我们通过蒙特卡洛近似用$ u_t $去求取$ Q_\pi(s,a) $。但是还有另一种办法，就是再添加一个神经网络$ q(s,a;w) $去拟合动作价值函数$ Q_\pi(s,a) $。这就是**Actor-Critic算法**。

在上一节DQN的相关内容中提到了Q网络的概念，但是它的神经网络$ Q(s,a;w) $和Actor-Critic所要训练的价值网络$ q(s,a;w) $的意义是不一样的。

1. $ Q(s,a;w) $是去近似$ Q_*(s,a) $，而$ q(s,a;w) $是去近似$ Q_\pi(s,a) $。
2. DQN用的是Q学习算法，属于异策略，而$ q(s,a;w) $属于同策略。

<font style="color:rgb(0,0,0);">Actor-critic 翻译成“演员—评委”方法。策略网络 </font>$ π(a|s; θ) $<font style="color:rgb(0,0,0);"> 相当于演员，它基于状态</font>$ s $<font style="color:rgb(0,0,0);"> </font>

<font style="color:rgb(0,0,0);">做出动作 </font>$ a $<font style="color:rgb(0,0,0);">。价值网络 </font>$ q(s, a; w) $<font style="color:rgb(0,0,0);"> 相当于评委，它给演员的表现打分，评价在状态 </font>$ s $<font style="color:rgb(0,0,0);">的情 </font>

<font style="color:rgb(0,0,0);">况下做出动作</font>$ a $<font style="color:rgb(0,0,0);"> 的好坏程度。策略网络（演员）和价值网络（评委）的关系如图:</font>

![](https://cdn.nlark.com/yuque/0/2024/png/49555563/1730194265843-af9635c0-68e0-4f0c-80af-fee219b2fc65.png)

<font style="color:rgb(0,0,0);">这里把奖励反馈给价值网络而不是策略网络的原因是：策略学习的目标函数</font>$ J(θ) $<font style="color:rgb(0,0,0);"> 是回报</font>$ U $<font style="color:rgb(0,0,0);">的期望，而不是奖励</font>$ R $<font style="color:rgb(0,0,0);">的期望；注意回报</font>$ U $<font style="color:rgb(0,0,0);">和奖励</font>$ R $<font style="color:rgb(0,0,0);">的区别。虽然能观测到当前的奖励</font>$ R $<font style="color:rgb(0,0,0);">，但是它对策略网络是毫无意义的；训练策略网络（演员）需要的是回报</font>$ U $<font style="color:rgb(0,0,0);">，而不是奖励</font>$ R $<font style="color:rgb(0,0,0);">。价值网络（评委）能够估算出回报</font>$ U $<font style="color:rgb(0,0,0);">的期望，因此能帮助训练策略网络（演员）。</font>

### 训练演员Actor
<font style="color:rgb(0,0,0);">策略网络（演员）想要改进自己的演技，但是演员自己不知道什么样的表演才算更好，所以需要价值网络（评委）的帮助。在演员做出动作 a 之后，评委会根据当前它的价值网络打一个分数 </font>$ \hat{q} $<font style="color:rgb(0,0,0);"> ≜ q(s, a; w)，并把分数反馈给演员，帮助演员做出改进。演员利用当前状态 s，自己的动作 a，以及评委的打分</font>$ \hat{q} $<font style="color:rgb(0,0,0);">，计算近似策略梯度，然后更新自己的参数</font>$ \theta $<font style="color:rgb(0,0,0);">（相当于改变自己的技术）。通过这种方式，演员的表现越来越受评委的好评，于是演员的获得的评分</font>$ \hat{q} $<font style="color:rgb(0,0,0);">越来越高。具体的梯度更新方法和之前类似，只是把</font>$ u_t $<font style="color:rgb(0,0,0);">换成了由价值网络</font>$ q(s, a; w) $<font style="color:rgb(0,0,0);">输出的数值。</font>

### 训练评委Critic
<font style="color:rgb(0,0,0);">通过以上分析，我们不难发现上述训练策略网络（演员）的方法不是真正让演员表现更好，只是让演员更迎合评委的喜好而已。因此，评委的水平也很重要，只有当评委的打分</font>$ \hat{q} $<font style="color:rgb(0,0,0);">真正反映出动作价值</font>$ Q_\pi $<font style="color:rgb(0,0,0);">，演员的水平才能真正提高。初始的时候，价值网络的参数</font>$ w $<font style="color:rgb(0,0,0);">是随机的，也就是说评委的打分是瞎猜。可以用</font>**<font style="color:rgb(0,0,0);"> SARSA算法（上一节最后的内容）</font>**<font style="color:rgb(0,0,0);">更新</font>$ w $<font style="color:rgb(0,0,0);">，提高评委的水平。每次从环境中观测到一个奖励</font>$ r $<font style="color:rgb(0,0,0);">，把</font>$ r $<font style="color:rgb(0,0,0);">看做是真相，用从环境中得到的奖励</font>$ r $<font style="color:rgb(0,0,0);">来校准评委的打分。</font>

下面概括 **Actor-Critic 训练流程**。设当前策略网络参数是 $ \theta_{\text{now}} $，价值网络参数是 $ w_{now} $。执行下面的步骤，将参数更新成 $ \theta_{new} $和 $ w_{new} $：

1. <font style="color:rgb(0,0,0);">观察到当前状态 </font>$ s_t $<font style="color:rgb(0,0,0);">，根据策略网络随机抽样得到动作</font>$ a_t $<font style="color:rgb(0,0,0);">：</font>$ a_t \sim \pi(\cdot | s_t; \theta_{\text{now}}) $<font style="color:rgb(0,0,0);">，并让智能体执行动作</font>$ a_t $<font style="color:rgb(0,0,0);">。</font>
2. <font style="color:rgb(0,0,0);">完成动作后，可以从环境中观测到奖励 </font>$ r_t $<font style="color:rgb(0,0,0);">和新的状态 </font>$ s_{t+1} $<font style="color:rgb(0,0,0);">。</font>
3. <font style="color:rgb(0,0,0);">再次根据策略网络随机抽样得到动作：</font>$ \tilde{a}_{t+1} \sim \pi(\cdot | s{t+1}; \theta_{\text{now}}) $<font style="color:rgb(0,0,0);">，这个动作只会用于计算，智能体不会执行动作 </font>$ \tilde{a}_{t+1} $<font style="color:rgb(0,0,0);">。</font>
4. <font style="color:rgb(0,0,0);">让价值网络打分：  
</font>$ \hat{q}_t = q(s_t, a_t;w_{now}) $<font style="color:rgb(0,0,0);">和 </font>$ \hat{q}_{t+1} = q(s_{t+1}, \tilde{a}_{t+1}; w_{now}) $<font style="color:rgb(0,0,0);">。</font>
5. <font style="color:rgb(0,0,0);">计算 TD 目标和 TD 误差：  
</font>$ \hat{y}t = r_t + \gamma \cdot \hat{q}_{t+1} $<font style="color:rgb(0,0,0);">和 </font>$ \delta_t = \hat{q}_t - \hat{y}_t $<font style="color:rgb(0,0,0);">。</font>
6. <font style="color:rgb(0,0,0);">使用</font>**<font style="color:rgb(0,0,0);">梯度下降</font>**<font style="color:rgb(0,0,0);">更新价值网络：  
</font>$ \mathbf{w}_{\text{new}} \leftarrow \mathbf{w}_{\text{now}} - \alpha \cdot \delta_t \cdot \nabla_{\mathbf{w}} q(s_t, a_t; \mathbf{w}_{\text{now}}) $<font style="color:rgb(0,0,0);">。</font>
7. <font style="color:rgb(0,0,0);">使用</font>**<font style="color:rgb(0,0,0);">梯度上升</font>**<font style="color:rgb(0,0,0);">更新策略网络：  
</font>$ \theta_{\text{new}} \leftarrow \theta_{\text{now}} + \beta \cdot \hat{q}t \cdot \nabla{\theta} \ln \pi(a_t | s_t; \theta_{\text{now}}) $<font style="color:rgb(0,0,0);">。</font>

<font style="color:rgb(0,0,0);">可以看出，在Actor-Critic算法中，策略网络和价值网络的训练是同时进行的，演员的表现会不断地符合评委的喜好，而评委也越来越专业，打分更加符合现实的需求，这样我们的演员也就能够越来越符合现实的需求（其策略能够使其获得更多的奖励）。</font>

## 6.策略梯度算法的原理
策略梯度算法会通过返回的奖励来调整策略，即在执行完一系列动作后，计算每个动作的累计回报$ u_t $(从该动作开始到回合结束的总奖励)。**如果某个动作导致的累计回报较大，意味着这个动作带来了更好的结果，应该更有可能被再次选择。因此，需要对策略进行更新，使得这一动作在未来遇到类似情况时应该以更大的概率被选中。**

**具体原理是：**

根据梯度计算公式：

$ \nabla J(\theta) = \mathbb{E} \left[ \sum_t \nabla_\theta \ln \pi_\theta(a_t | s_t) \cdot Q_\pi(s_t,a_t) \right]

 $

其中包含了$ \nabla_\theta \ln \pi(a_t| s_t;\theta) $这一项，根据梯度的定义，当$ \nabla_\theta \ln \pi(a_t| s_t;\theta)>0 $时，增大$ \theta $可以增加使在状态$ s_t $下选择动作$ a_t $的概率；当$ \nabla_\theta \ln \pi(a_t| s_t;\theta)<0 $时，减小$ \theta $可以增加使在状态$ s_t $下选择动作$ a_t $的概率。由于我们采用的是梯度上升的方式，当一系列的动作使得奖励$ u_t $（根据REINFORCE算法，$ u_t $可以近似等于$ Q_\pi(s_t,a_t) $，或者是由价值网络输出的分数也行）很大时，这里有两种情况：



**1.**$ \nabla_\theta \ln \pi(a_t| s_t;\theta)>0 $**：**

由于$ u_t $是一个大于0的很大的数，两者相乘肯定是一个很大的正数，因此$ \nabla J(\theta)>0 $，根据梯度上升公式，$ \theta $会变得更大。再根据我们刚刚的分析，当$ \nabla_\theta \ln \pi(a_t| s_t;\theta)>0 $时，增大$ \theta $可以增加使在状态$ s_t $下选择动作$ a_t $的概率。因此，此次更新的结果是，在状态$ s_t $下选择动作$ a_t $的概率会被增加。

**2.**$ \nabla_\theta \ln \pi(a_t| s_t;\theta)<0 $**：**

由于$ u_t $是一个大于0的很大的数，两者相乘肯定是一个很大的负数，因此$ \nabla J(\theta)<0 $，根据梯度上升公式，$ \theta $会变得更小。再根据我们刚刚的分析，当$ \nabla_\theta \ln \pi(a_t| s_t;\theta)<0 $时，减小$ \theta $可以增加使在状态$ s_t $下选择动作$ a_t $的概率。因此，此次更新的结果是，在状态$ s_t $下选择动作$ a_t $的概率会被增加。



所以，不管当前的$ \nabla_\theta \ln \pi(a_t| s_t;\theta) $是什么，只要当前的一系列动作可以得到很高的回报，梯度更新出来的参数都会使在状态$ s_t $下选择动作$ a_t $的概率增加。这就是策略梯度的逻辑上的原理：机器人在训练时会记录经历过的状态，执行过的动作和得到过的奖励，如果这一个训练回合获得的回报很高，那么就会在以后增大这个回合中经历的所有状态下执行这个回合中与状态对应的动作的概率。









